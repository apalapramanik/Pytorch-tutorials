# -*- coding: utf-8 -*-
"""pytorch1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D1NOibbPoYo178C0OvxoKl97jv5KAWbr
"""

#https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html

import torch
import numpy as np

#create tensor from data:
data = [[1,2],[3,4]]
x_data = torch.tensor(data)

#create tensor from numpy array:
np_array = np.array(data)
x_np = torch.from_numpy(np_array)

#create tensor from another tensor:
x_ones = torch.ones_like(x_data) #retains properties of x_data
print(f"Ones Tensor: \n {x_ones} \n")

x_rand = torch.rand_like(x_data, dtype=torch.float)#overrides data type of x_data to float
print(f"Random tensor: \n {x_rand} \n")

x_zeros = torch.zeros_like(x_data) #retains properties of x_data
print(f"Zeros Tensor: \n {x_zeros} \n")

shape = (2,3,) #declare shape of tensor as row, column
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)

print(f"Random Tensor: \n {rand_tensor} \n")
print(f"Ones Tensor: \n {ones_tensor} \n")
print(f"Zeros Tensor: \n {zeros_tensor}")
print()

#tensor attributes:
tensor = torch.rand(3,4)

print(f"shape: {tensor.shape}")
print(f"datatype: {tensor.dtype}")
print(f"storage: {tensor.device}")

#tensor operations:

#check for gpu:
if torch.cuda.is_available():
  tensor = tensor.to('cuda')
  print(f"device:{tensor.device}")

#standard numpy like indexing and slicing:

tensor = torch.ones(4,4)
tensor[:,1] = 0
print(tensor)

#joining tensors using cat

t1 = torch.cat([tensor,tensor,tensor], dim=1)
print(t1)

#tensor multiplication:
print(f"tensor multiplication:\n{tensor.mul(tensor)}\n") #one syntax
print(f"tensor multiplication:\n {tensor*tensor}\n")#second syntax
print(f"tensor multiplication:\n {tensor*5}\n")#second syntax

# matrix multiplication between tensors
print(f"tensor.matmul(tensor.T) \n {tensor.matmul(tensor.T)} \n")
# Alternative syntax:
print(f"tensor @ tensor.T \n {tensor @ tensor.T}")

# in place operations: operations with suffix "_"
print(tensor,"\n")
tensor.add_(5)
print(tensor)
#In-place operations save some memory, 
#but can be problematic when computing derivatives 
#because of an immediate loss of history. 
#Hence, their use is discouraged

#bridge with numpy:

#tensor to numpy array:
t = torch.ones(5)
print(f"tensor: {t}\n")
n = t.numpy()
print(f"numpy:{n}\n")

#change is reflected:
t.add_(1)
print(f"t: {t}")
print(f"n: {n}")

#numpy to tensor:
n = np.ones(5)
t = torch.from_numpy(n)

np.add(n, 1, out=n)
print(f"t: {t}")
print(f"n: {n}")